[[getting-started]]
= Getting Started

== Deploying Streams on Nomad

The following guide assumes that you have a https://www.nomadproject.io/[Nomad 0.5+] cluster available.
If you do not have a Nomad cluster available, see the next section which describes running a local Nomad for development/testing
otherwise continue to <<installing-scdf>>.

=== A local Nomad cluster with Vagrant Hashistack

There are a few ways to stand up a local Nomad cluster on your machine for testing.
For the purpose of this guide, the https://github.com/donovanmuller/hashistack-vagrant[hashistack-vagrant] project will be used.

[IMPORTANT]
====
The `hashistack-vagrant` VM is configured by default with `2048 MB` of memory and `2` CPUs.
If you run into issues with job allocations failing because of resource starvation, you can tweak the
memory and CPU configuration in the `Vagrantfile`.

Please see the https://github.com/donovanmuller/spring-cloud-dataflow-server-nomad/wiki/Resource-Allocations[Resource Allocations]
section in the `spring-cloud-dataflow-server-nomad` project Wiki for more information.
====

==== Installation and Getting Started

Follow the https://github.com/donovanmuller/hashistack-vagrant#quickstart[Quickstart]
section of the https://github.com/donovanmuller/hashistack-vagrant[hashistack-vagrant].

Once you have successfully started the Vagrant VM and (with `tmuxp load full-hashistack.yml`) "hashistack",
make sure you can use the `nomad` client to query the local instance:

[source,console]
----
vagrant@hashistack:~$ nomad status
No running jobs
----

[NOTE]
====
You could also install the `nomad` binary locally and connect to the Nomad client running inside the VM. For example
on Mac you could install Nomad with homebrew and add the `--address` option to your commands:

[subs="attributes"]
[source,console]
----
$ brew install nomad
...
$ nomad status --address=http://172.16.0.2:4646
ID    Type     Priority  Status
scdf  service  50        running
----
====

[[installing-scdf]]
=== Installing the Data Flow Server for Nomad

To install a Data Flow Server and supporting infrastructure components to Nomad we will use the provided Job specification provided
in the https://github.com/donovanmuller/spring-cloud-dataflow-server-nomad/tree/{scdf-server-nomad-version}/src/etc/nomad[`src/etc/nomad/`]
directory of the project's GitHub repository.

NOTE: This Job requires the https://www.nomadproject.io/docs/drivers/docker.html[Docker] driver.

This job specification includes the following tasks:

* Spring Cloud Data Flow Server for Nomad
* Spring Cloud Data Flow Metrics Collector
* MySQL - as the datasource for the Data Flow Server
* Redis - for analytics support
* Kafka - as the Spring Cloud Stream default binder implementation

TIP: If you are not using the hashistack-vagrant VM, please adjust the `region` and `datacenters`
values in the `scdf.nomad` job specification file accordingly.

Next, using the SSH session to the VM, run the job with:

[subs="attributes"]
[source,console]
----
vagrant@hashistack:~$ nomad run https://raw.githubusercontent.com/donovanmuller/spring-cloud-dataflow-server-nomad/{scdf-server-nomad-version}/src/etc/nomad/nexus.nomad
==> Monitoring evaluation "491168de"
    Evaluation triggered by job "scdf"
    Allocation "63c64eaf" created: node "774539c7", group "scdf"
    Evaluation status changed: "pending" -> "complete"
==> Evaluation "491168de" finished with status "complete"
----

Allow some time for the Docker images to be pulled and all containers to be started.
You can verify that all tasks have been successfully started by checking the
corresponding allocation status:

[source,console]
----
vagrant@hashistack:~$ nomad alloc-status 63c64eaf
ID                  = 63c64eaf
Eval ID             = 491168de
Name                = scdf.scdf[0]
Node ID             = 774539c7
Job ID              = scdf
Client Status       = running
Client Description  = <none>
Desired Status      = run
Desired Description = <none>
Created At          = 08/01/17 20:58:44 UTC

Task "kafka" is "running"
Task Resources
CPU         Memory           Disk     IOPS  Addresses
25/500 MHz  507 MiB/512 MiB  500 MiB  0     kafka: 172.16.0.2:9092

Recent Events:
Time                   Type        Description
08/01/17 21:02:16 UTC  Started     Task started by client
08/01/17 21:01:59 UTC  Restarting  Task restarting in 16.424762972s
08/01/17 21:01:59 UTC  Terminated  Exit Code: 1, Exit Message: "Docker container exited with non-zero exit code: 1"
08/01/17 21:01:58 UTC  Started     Task started by client
08/01/17 21:01:40 UTC  Restarting  Task restarting in 16.967973166s
08/01/17 21:01:40 UTC  Terminated  Exit Code: 1, Exit Message: "Docker container exited with non-zero exit code: 1"
08/01/17 21:01:38 UTC  Started     Task started by client
08/01/17 20:58:44 UTC  Driver      Downloading image wurstmeister/kafka:0.10.1.0
08/01/17 20:58:44 UTC  Task Setup  Building Task Directory
08/01/17 20:58:44 UTC  Received    Task received by client

Task "mysql" is "running"
Task Resources
CPU        Memory           Disk     IOPS  Addresses
2/500 MHz  117 MiB/128 MiB  500 MiB  0     db: 172.16.0.2:3306

Recent Events:
Time                   Type        Description
08/01/17 21:00:50 UTC  Started     Task started by client
08/01/17 20:58:44 UTC  Driver      Downloading image mysql:5.6
08/01/17 20:58:44 UTC  Task Setup  Building Task Directory
08/01/17 20:58:44 UTC  Received    Task received by client

Task "redis" is "running"
Task Resources
CPU        Memory          Disk     IOPS  Addresses
3/256 MHz  6.2 MiB/64 MiB  500 MiB  0     redis: 172.16.0.2:6379

Recent Events:
Time                   Type        Description
08/01/17 20:58:57 UTC  Started     Task started by client
08/01/17 20:58:44 UTC  Driver      Downloading image redis:3-alpine
08/01/17 20:58:44 UTC  Task Setup  Building Task Directory
08/01/17 20:58:44 UTC  Received    Task received by client

Task "scdf-metrics-collector" is "running"
Task Resources
CPU          Memory           Disk     IOPS  Addresses
518/500 MHz  507 MiB/512 MiB  500 MiB  0     http: 172.16.0.2:8080

Recent Events:
Time                   Type        Description
08/01/17 21:02:10 UTC  Started     Task started by client
08/01/17 21:01:54 UTC  Restarting  Task restarting in 15.187972956s
08/01/17 21:01:54 UTC  Terminated  Exit Code: 1, Exit Message: "Docker container exited with non-zero exit code: 1"
08/01/17 21:01:34 UTC  Started     Task started by client
08/01/17 21:01:17 UTC  Restarting  Task restarting in 16.424762972s
08/01/17 21:01:17 UTC  Terminated  Exit Code: 1, Exit Message: "Docker container exited with non-zero exit code: 1"
08/01/17 21:00:57 UTC  Started     Task started by client
08/01/17 21:00:39 UTC  Restarting  Task restarting in 16.967973166s
08/01/17 21:00:39 UTC  Terminated  Exit Code: 1, Exit Message: "Docker container exited with non-zero exit code: 1"
08/01/17 21:00:16 UTC  Started     Task started by client

Task "scdf-server" is "running"
Task Resources
CPU        Memory           Disk     IOPS  Addresses
5/500 MHz  323 MiB/384 MiB  500 MiB  0     http: 172.16.0.2:9393

Recent Events:
Time                   Type            Description
08/01/17 21:11:26 UTC  Started         Task started by client
08/01/17 21:11:08 UTC  Restarting      Task restarting in 17.905628447s
08/01/17 21:11:08 UTC  Terminated      Exit Code: 137, Exit Message: "Docker container exited with non-zero exit code: 137"
08/01/17 21:05:27 UTC  Started         Task started by client
08/01/17 21:05:09 UTC  Restarting      Task restarting in 18.119676483s
08/01/17 21:05:09 UTC  Driver Failure  failed to initialize task "scdf-server" for alloc "63c64eaf-5a43-3e18-8e55-0134f8c18f93": Failed to pull `donovanmuller/spring-cloud-dataflow-server-nomad:1.2.3.RELEASE`: API error (404): {"message":"manifest for donovanmuller/spring-cloud-dataflow-server-nomad:1.2.3.RELEASE not found"}
08/01/17 21:05:05 UTC  Driver          Downloading image donovanmuller/spring-cloud-dataflow-server-nomad:1.2.3.RELEASE
08/01/17 21:04:47 UTC  Restarting      Task restarting in 17.088216893s
08/01/17 21:04:47 UTC  Driver Failure  failed to initialize task "scdf-server" for alloc "63c64eaf-5a43-3e18-8e55-0134f8c18f93": Failed to pull `donovanmuller/spring-cloud-dataflow-server-nomad:1.2.3.RELEASE`: API error (404): {"message":"manifest for donovanmuller/spring-cloud-dataflow-server-nomad:1.2.3.RELEASE not found"}
08/01/17 21:04:42 UTC  Driver          Downloading image donovanmuller/spring-cloud-dataflow-server-nomad:1.2.3.RELEASE

Task "zookeeper" is "running"
Task Resources
CPU        Memory          Disk     IOPS  Addresses
2/500 MHz  45 MiB/128 MiB  500 MiB  0     zookeeper: 172.16.0.2:2181
                                          follower: 172.16.0.2:2888
                                          leader: 172.16.0.2:3888

Recent Events:
Time                   Type        Description
08/01/17 21:02:12 UTC  Started     Task started by client
08/01/17 20:58:44 UTC  Driver      Downloading image digitalwonderland/zookeeper:latest
08/01/17 20:58:44 UTC  Task Setup  Building Task Directory
08/01/17 20:58:44 UTC  Received    Task received by client

...
----

or alternatively check the health status of all services using the Consul UI:

image::{scdf-server-nomad-asciidoc}/images/scdf-nomad-up.jpg[Data Flow Server and components up]

[NOTE]
====
If you are using a local `nomad` binary you can reference the remote `scdf.nomad` file directly.

[subs="attributes"]
[source,console]
----
$ nomad run --address=http://172.16.0.2:4646 https://raw.githubusercontent.com/donovanmuller/spring-cloud-dataflow-server-nomad/{scdf-server-nomad-version}/src/etc/nomad/scdf.nomad
...
----

====

=== Download and run the Spring Cloud Data Flow Shell

Download and run the Shell, targeting the Data Flow Server exposed via a Fabio route.

[subs="attributes"]
[source,console]
----
$ wget http://repo.spring.io/{dataflow-version-type-lowercase}/org/springframework/cloud/spring-cloud-dataflow-shell/{dataflow-project-version}/spring-cloud-dataflow-shell-{dataflow-project-version}.jar
$ java -jar spring-cloud-dataflow-shell-{dataflow-project-version}.jar \
  --dataflow.uri=http://scdf-server.hashistack.vagrant/ \
  --dataflow.username=user \
  --dataflow.password=password

  ____                              ____ _                __
 / ___| _ __  _ __(_)_ __   __ _   / ___| | ___  _   _  __| |
 \___ \| '_ \| '__| | '_ \ / _` | | |   | |/ _ \| | | |/ _` |
  ___) | |_) | |  | | | | | (_| | | |___| | (_) | |_| | (_| |
 |____/| .__/|_|  |_|_| |_|\__, |  \____|_|\___/ \__,_|\__,_|
  ____ |_|    _          __|___/                 __________
 |  _ \  __ _| |_ __ _  |  ___| | _____      __  \ \ \ \ \ \
 | | | |/ _` | __/ _` | | |_  | |/ _ \ \ /\ / /   \ \ \ \ \ \
 | |_| | (_| | || (_| | |  _| | | (_) \ V  V /    / / / / / /
 |____/ \__,_|\__\__,_| |_|   |_|\___/ \_/\_/    /_/_/_/_/_/

{dataflow-project-version}

Welcome to the Spring Cloud Data Flow shell. For assistance hit TAB or type "help".
dataflow:>
----

[NOTE]
====
Security is enabled by default. See the link:http://docs.spring.io/spring-cloud-dataflow/docs/1.2.3.RELEASE/reference/htmlsingle/#configuration-security-single-user-authentication[Security configuration section] for more information.
There are two default users created:

[cols=2*,options="header"]
|===
|Username
|Password

|user | password
|admin | admin
|===
====

=== Registering Stream applications with Docker resource

Now register all out-of-the-box stream applications using the Docker resource type, built with the Kafka binder in bulk with the following command.

TIP: For more details, review how to link:http://docs.spring.io/spring-cloud-dataflow/docs/{scdf-core-version}/reference/html/spring-cloud-dataflow-register-apps.html[register applications].

[source,console]
----
dataflow:>app import --uri http://bit.ly/Bacon-RELEASE-stream-applications-kafka-10-docker
Successfully registered applications: [source.tcp, sink.jdbc, source.http, sink.rabbit, source.rabbit, source.ftp, sink.gpfdist, processor.transform, source.loggregator, source.sftp, processor.filter, sink.cassandra, processor.groovy-filter, sink.router, source.trigger, sink.hdfs-dataset, processor.splitter, source.load-generator, processor.tcp-client, source.time, source.gemfire, source.twitterstream, sink.tcp, source.jdbc, sink.field-value-counter, sink.redis-pubsub, sink.hdfs, processor.bridge, processor.pmml, processor.httpclient, source.s3, sink.ftp, sink.log, sink.gemfire, sink.aggregate-counter, sink.throughput, source.triggertask, sink.s3, source.gemfire-cq, source.jms, source.tcp-client, processor.scriptable-transform, sink.counter, sink.websocket, source.mongodb, source.mail, processor.groovy-transform, source.syslog]
----

=== Deploy a simple stream in the shell

Create a simple `ticktock` stream definition and deploy it immediately using the following command:

[source,console]
----
dataflow:>stream create --name ticktock --definition "time | log" --deploy
Created new stream 'ticktock'
Deployment request has been sent
----

Verify the deployed apps using the by checking the status of the apps using the Shell:

image::{scdf-server-nomad-asciidoc}/images/scdf-nomad-stream-deployed.jpg[ticktock streamd deployed]

To verify that the stream is working as expected, tail the logs of the `ticktock-log` using `nomad`:

[source,console]
----
vagrant@hashistack:~$ nomad logs 71f7aba1
...
...  INFO 1 --- [afka-listener-1] log-sink                                 : 02/08/17 14:49:59
...  INFO 1 --- [afka-listener-1] log-sink                                 : 02/08/17 14:50:01
...  INFO 1 --- [afka-listener-1] log-sink                                 : 02/08/17 14:50:02
...  INFO 1 --- [afka-listener-1] log-sink                                 : 02/08/17 14:50:03
...  INFO 1 --- [afka-listener-1] log-sink                                 : 02/08/17 14:50:04
...  INFO 1 --- [afka-listener-1] log-sink                                 : 02/08/17 14:50:05
...  INFO 1 --- [afka-listener-1] log-sink                                 : 02/08/17 14:50:06
...
----

=== Registering Stream applications with Maven resource

The Data Flow Server for Nomad also supports apps registered with a Maven resource URI
in addition to the Docker resource type. Using the `ticktock` stream example above, we will create a similar stream definition
but using the Maven resource versions of the apps.

For this example we will register the apps individually using the following command:

[subs="attributes"]
[source,console]
----
dataflow:>app register --type source --name time-mvn --uri maven://org.springframework.cloud.stream.app:time-source-kafka:{dataflow-project-version}
Successfully registered application 'source:time-mvn'
dataflow:>app register --type sink --name log-mvn --uri maven://org.springframework.cloud.stream.app:log-sink-kafka:{dataflow-project-version}
Successfully registered application 'sink:log-mvn'
----

NOTE: We couldn't bulk import the Maven version of the apps as we did for the Docker versions because the app names
would conflict, as the names defined in the bulk import files are the same across resource types. Hence we register the
Maven apps with a `-mvn` suffix.

=== Deploy a simple stream in the shell

Create a simple `ticktock-mvn` stream definition and deploy it immediately using the following command:

[source,console]
----
dataflow:>stream create --name ticktock-mvn --definition "time-mvn | log-mvn" --deploy
Created new stream 'ticktock-mvn'
Deployment request has been sent
----

NOTE: There could be a slight delay once the above command is issued. This is due to the Maven artifacts being
resolved and cached locally. Depending on the size of the artifacts, this could take some time.

To verify that the stream is working as expected, tail the logs of the `ticktock-mvn-log-mvn` using `nomad`:

[source,console]
----
$ nomad logs -f 3f474cc7
...
...  INFO 1 --- [afka-listener-1] log-sink                                 : 02/08/17 18:34:23
...  INFO 1 --- [afka-listener-1] log-sink                                 : 02/08/17 18:34:25
...  INFO 1 --- [afka-listener-1] log-sink                                 : 02/08/17 18:34:26
...  INFO 1 --- [afka-listener-1] log-sink                                 : 02/08/17 18:34:27
----

== Deploying Tasks on Nomad

Deploying Task applications using the Data Flow Server for Nomad is a similar affair to deploying Stream apps.
Therefore, for brevity, only the Maven resource version of the task will be shown as an example.

=== Registering Task application with Maven resource

This time we will bulk import the Task application, as we do not have any Docker resource versions imported which would cause conflicts in naming.
Import all Maven task applications with the following command:

[source,console]
----
dataflow:>app import --uri http://bit.ly/Belmont-GA-task-applications-maven
----

=== Launch a simple task in the shell

Let’s create a simple task definition and launch it.

[source,console]
----
dataflow:>task create task1 --definition "timestamp"
dataflow:>task launch task1
----

Verify that the task executed successfully by executing these commands:

[source,console]
----
dataflow:>task list
╔═════════╤═══════════════╤═══════════╗
║Task Name│Task Definition│Task Status║
╠═════════╪═══════════════╪═══════════╣
║task1    │timestamp      │unknown    ║
╚═════════╧═══════════════╧═══════════╝

dataflow:>task execution list
╔═════════╤══╤═════════════════════════════╤═════════════════════════════╤═════════╗
║Task Name│ID│         Start Time          │          End Time           │Exit Code║
╠═════════╪══╪═════════════════════════════╪═════════════════════════════╪═════════╣
║task1    │1 │Wed Aug 02 15:34:01 SAST 2017│Wed Aug 02 15:34:01 SAST 2017│0        ║
╚═════════╧══╧═════════════════════════════╧═════════════════════════════╧═════════╝
----

You can also view the task execution status by using the Data Flow Server UI.

==== Cleanup completed tasks

If you want to delete the Build and Pod created by this task execution, execute the following:

[source,console]
----
dataflow:>task destroy --name task1
----
